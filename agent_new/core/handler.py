#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ¸å¿ƒå¤„ç†é€»è¾‘ - ç»Ÿä¸€å¤„ç† REST å’Œ WebSocket è¯·æ±‚
"""
import logging
from typing import Dict, Any, List, Optional, Callable, Awaitable

from config import settings
from prompts import load_prompt
from core.llm import llm_manager, format_messages
from core.intent import recognize_intent, needs_expert, is_device_control, is_casual_chat
from core.query_rewriter import rewrite_query
from services.expert_consultation_service import expert_service
from services.device_expert_service import device_expert_service
from services.chat_history_service import save_message, get_history, format_history_for_llm

logger = logging.getLogger(__name__)


class ChatHandler:
    """
    èŠå¤©å¤„ç†å™¨ - æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
    
    å¤„ç†æµç¨‹:
    1. æ„å›¾è¯†åˆ«
    2. æ ¹æ®æ„å›¾è·¯ç”±åˆ°ä¸åŒå¤„ç†åˆ†æ”¯
    3. æ‰€æœ‰ç»“æœæœ€ç»ˆé€šè¿‡ thinking agent æ•´åˆè¾“å‡º
    """
    
    def __init__(self):
        pass
    
    async def process(
        self,
        query: str,
        session_id: str,
        context: Optional[Dict[str, Any]] = None,
        stream_callback: Optional[Callable[[str], Awaitable[None]]] = None,
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·è¯·æ±‚
        
        Args:
            query: ç”¨æˆ·è¾“å…¥
            session_id: ä¼šè¯ ID
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            stream_callback: æµå¼å›è°ƒå‡½æ•°ï¼ˆç”¨äº WebSocketï¼‰
            
        Returns:
            Dict: å¤„ç†ç»“æœ
        """
        logger.info(f"ğŸš€ æ”¶åˆ°è¯·æ±‚ | session={session_id} | query={query[:30]}...")
        context = context or {}
        
        # 1. è·å–å†å²å¯¹è¯è®°å½•
        history_records = get_history(session_id, limit=20)
        history = format_history_for_llm(history_records)
        
        # 2. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        save_message(session_id=session_id, role="user", message=query)
        
        # 3. æ„å›¾è¯†åˆ«
        intent, intent_stats = await recognize_intent(query, history)
        logger.info(f"ğŸ¯ æ„å›¾: {intent}")
        
        # 4. æ ¹æ®æ„å›¾å¤„ç†
        response_content = ""
        metadata = {"intent": intent}
        
        try:
            if is_device_control(intent):
                # è®¾å¤‡æ§åˆ¶åˆ†æ”¯
                logger.info(f"â†’ è®¾å¤‡æ§åˆ¶")
                response_content, metadata = await self._handle_device_control(
                    query=query,
                    session_id=session_id,
                    context=context,
                    history=history,
                    stream_callback=stream_callback,
                )
            elif needs_expert(intent):
                # æ•°æ®æŸ¥è¯¢/åˆ†æåˆ†æ”¯ - éœ€è¦ä¸“å®¶
                logger.info(f"â†’ æ•°æ®ä¸“å®¶")
                response_content, metadata = await self._handle_expert_query(
                    query=query,
                    session_id=session_id,
                    context=context,
                    history=history,
                    intent=intent,
                    stream_callback=stream_callback,
                )
            else:
                # é—²èŠåˆ†æ”¯
                logger.info(f"â†’ é—²èŠ")
                response_content, metadata = await self._handle_casual_chat(
                    query=query,
                    context=context,
                    history=history,
                    stream_callback=stream_callback,
                )
            
            metadata["intent"] = intent
            
        except Exception as e:
            logger.error(f"å¤„ç†è¯·æ±‚å¤±è´¥: {e}", exc_info=True)
            response_content = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
            metadata["error"] = str(e)
        
        # 5. ä¿å­˜ AI å›ç­”
        save_message(
            session_id=session_id,
            role="assistant",
            message=response_content,
            intent=intent,
            metadata=metadata,
        )
        
        logger.info(f"âœ… å®Œæˆ | intent={intent}")
        
        return {
            "status": "success" if "error" not in metadata else "error",
            "response": response_content,
            "intent": intent,
            "session_id": session_id,
            "metadata": metadata,
        }
    
    async def _handle_device_control(
        self,
        query: str,
        session_id: str,
        context: Dict[str, Any],
        history: List[Dict[str, str]],
        stream_callback: Optional[Callable[[str], Awaitable[None]]] = None,
    ) -> tuple[str, Dict[str, Any]]:
        """å¤„ç†è®¾å¤‡æ§åˆ¶è¯·æ±‚"""
        
        if not settings.ENABLE_DEVICE_EXPERT:
            return "è®¾å¤‡æ§åˆ¶åŠŸèƒ½æœªå¯ç”¨ï¼Œè¯·è”ç³»ç®¡ç†å‘˜", {"device_expert_enabled": False}
        
        try:
            # è°ƒç”¨è®¾å¤‡ä¸“å®¶æœåŠ¡
            device_response = await device_expert_service.consult(
                query=query,
                session_id=session_id,
                context=context,
                stream_callback=stream_callback,
            )
            
            if device_response.get("success"):
                # æå–è®¾å¤‡ä¸“å®¶çš„å›ç­”
                result = device_response.get("result", {})
                messages = result.get("messages", [])
                if messages:
                    device_answer = messages[0].get("content", "è®¾å¤‡æ“ä½œå®Œæˆ")
                else:
                    device_answer = "è®¾å¤‡æ“ä½œå®Œæˆ"
                
                logger.info(f"â†’ thinkingæ•´åˆ")
                # é€šè¿‡ thinking agent æ•´åˆè¾“å‡º
                final_response = await self._thinking_integrate(
                    user_query=query,
                    raw_answer=device_answer,
                    source="è®¾å¤‡ä¸“å®¶",
                    context=context,
                    history=history,
                    stream_callback=stream_callback,
                )
                
                return final_response, {
                    "device_expert_used": True,
                    "device_type": device_response.get("device_type"),
                    "success": True,
                }
            else:
                error = device_response.get("error", "æœªçŸ¥é”™è¯¯")
                logger.error(f"âŒ è®¾å¤‡ä¸“å®¶å¤±è´¥: {error}")
                return f"æŠ±æ­‰ï¼Œè®¾å¤‡æ“ä½œå¤±è´¥ï¼š{error}", {
                    "device_expert_used": True,
                    "success": False,
                    "error": error,
                }
                
        except Exception as e:
            logger.error(f"è®¾å¤‡æ§åˆ¶å¤±è´¥: {e}", exc_info=True)
            return f"æŠ±æ­‰ï¼Œè®¾å¤‡æ§åˆ¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}", {"error": str(e)}
    
    async def _handle_expert_query(
        self,
        query: str,
        session_id: str,
        context: Dict[str, Any],
        history: List[Dict[str, str]],
        intent: str,
        stream_callback: Optional[Callable[[str], Awaitable[None]]] = None,
    ) -> tuple[str, Dict[str, Any]]:
        """å¤„ç†éœ€è¦ä¸“å®¶çš„æ•°æ®æŸ¥è¯¢/åˆ†æè¯·æ±‚"""
        
        # æŸ¥è¯¢é‡å†™
        rewritten_query, rewrite_stats = await rewrite_query(
            query=query,
            history=history,
            context=context,
        )
        
        if not settings.ENABLE_EXPERT_CONSULTATION:
            # ä¸“å®¶æœªå¯ç”¨ï¼Œä½¿ç”¨å…œåº•
            logger.warning(f"âš ï¸ æ•°æ®ä¸“å®¶æœªå¯ç”¨")
            return await self._handle_casual_chat(
                query=rewritten_query,
                context=context,
                history=history,
                stream_callback=stream_callback,
            )
        
        try:
            # è°ƒç”¨ä¸“å®¶æœåŠ¡
            expert_config = {
                "rag": {
                    "collection_name": "japan_shrimp",
                    "topk_single": 5,
                    "topk_multi": 5
                },
                "mode": "single",
                "single": {
                    "temperature": 0.4,
                    "system_prompt": "ä½ æ˜¯ä¸€ä¸ªæ—¥æœ¬é™†ä¸Šå…»æ®–é¢†åŸŸçš„ä¸“å®¶ï¼Œä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»“åˆå¢å¼ºæ£€ç´¢åçš„ç›¸å…³çŸ¥è¯†ï¼Œè¿›è¡Œæ•°æ®æŸ¥è¯¢ã€èšåˆåˆ†æï¼Œå¹¶ç»™å‡ºä¸“ä¸šçš„ç»“è®ºå’Œå»ºè®®ã€‚",
                    "max_tokens": 4096
                }
            }
            
            expert_response = await expert_service.consult(
                query=rewritten_query,
                context={
                    "original_query": query,
                    "intent": intent,
                    **context,
                },
                session_id=session_id,
                config=expert_config,
            )
            
            if expert_response.get("success"):
                expert_answer = expert_response.get("answer", "")
                
                logger.info(f"â†’ thinkingæ•´åˆ")
                # é€šè¿‡ thinking agent æ•´åˆè¾“å‡º
                final_response = await self._thinking_integrate(
                    user_query=query,
                    raw_answer=expert_answer,
                    source="å…»æ®–ä¸“å®¶",
                    context=context,
                    history=history,
                    stream_callback=stream_callback,
                )
                
                return final_response, {
                    "expert_consulted": True,
                    "confidence": expert_response.get("confidence", 0.0),
                }
            else:
                # ä¸“å®¶å’¨è¯¢å¤±è´¥ï¼Œä½¿ç”¨å…œåº•
                logger.warning(f"âš ï¸ æ•°æ®ä¸“å®¶å¤±è´¥: {expert_response.get('error')}")
                return await self._handle_casual_chat(
                    query=rewritten_query,
                    context=context,
                    history=history,
                    stream_callback=stream_callback,
                )
                
        except Exception as e:
            logger.error(f"ä¸“å®¶å’¨è¯¢å¤±è´¥: {e}", exc_info=True)
            return await self._handle_casual_chat(
                query=rewritten_query,
                context=context,
                history=history,
                stream_callback=stream_callback,
            )
    
    async def _handle_casual_chat(
        self,
        query: str,
        context: Dict[str, Any],
        history: List[Dict[str, str]],
        stream_callback: Optional[Callable[[str], Awaitable[None]]] = None,
    ) -> tuple[str, Dict[str, Any]]:
        """å¤„ç†é—²èŠè¯·æ±‚"""
        
        # åŠ è½½é—²èŠæç¤ºè¯
        system_prompt = load_prompt("chat")
        
        # æ„å»ºæ¶ˆæ¯
        messages = format_messages(
            system_prompt=system_prompt,
            user_message=query,
            history=history,
        )
        
        # è°ƒç”¨ LLM
        response = await llm_manager.invoke(
            messages=messages,
            stream=stream_callback is not None,
            stream_callback=stream_callback,
        )
        
        return response, {"chat_agent_used": True}
    
    async def _thinking_integrate(
        self,
        user_query: str,
        raw_answer: str,
        source: str,
        context: Dict[str, Any],
        history: List[Dict[str, str]],
        stream_callback: Optional[Callable[[str], Awaitable[None]]] = None,
    ) -> str:
        """
        é€šè¿‡ thinking agent æ•´åˆè¾“å‡º
        
        æ‰€æœ‰å¤–éƒ¨æœåŠ¡ï¼ˆä¸“å®¶ã€è®¾å¤‡ï¼‰çš„å›ç­”éƒ½é€šè¿‡è¿™é‡Œæ•´åˆï¼Œ
        ç¡®ä¿è¾“å‡ºæ ¼å¼ä¸€è‡´ã€ä¸“ä¸šã€‚
        
        Args:
            user_query: ç”¨æˆ·åŸå§‹é—®é¢˜
            raw_answer: åŸå§‹å›ç­”ï¼ˆæ¥è‡ªä¸“å®¶æˆ–è®¾å¤‡æœåŠ¡ï¼‰
            source: å›ç­”æ¥æºï¼ˆå¦‚"å…»æ®–ä¸“å®¶"ã€"è®¾å¤‡ä¸“å®¶"ï¼‰
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            history: å¯¹è¯å†å²
            stream_callback: æµå¼å›è°ƒ
            
        Returns:
            str: æ•´åˆåçš„å›ç­”
        """
        # åŠ è½½ thinking æç¤ºè¯
        system_prompt = load_prompt("thinking")
        
        # æ„å»ºæ•´åˆæç¤º
        user_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{user_query}

{source}å›ç­”ï¼š
{raw_answer}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œæ•´åˆå¹¶ä¼˜åŒ–å›ç­”ï¼Œç¡®ä¿ï¼š
1. å›ç­”ä¸“ä¸šã€å‡†ç¡®
2. æ ¼å¼æ¸…æ™°ã€æ˜“è¯»
3. å¦‚æœ‰å¿…è¦ï¼Œè¡¥å……å¼•å¯¼é—®é¢˜"""
        
        # æ„å»ºæ¶ˆæ¯
        messages = format_messages(
            system_prompt=system_prompt,
            user_message=user_prompt,
            history=history,
        )
        
        # è°ƒç”¨ LLM
        response = await llm_manager.invoke(
            messages=messages,
            stream=stream_callback is not None,
            stream_callback=stream_callback,
        )
        
        return response


# å…¨å±€å¤„ç†å™¨å®ä¾‹
chat_handler = ChatHandler()


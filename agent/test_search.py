#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯• LLM è”ç½‘æœç´¢åŠŸèƒ½ - ä½¿ç”¨ç›¸åŒé—®é¢˜å¯¹æ¯”æœç´¢æ•ˆæœ
"""
import asyncio
from agents.llm_utils import execute_llm_call, execute_search_call, LLMConfig, format_messages_for_llm
from langchain_core.messages import HumanMessage

# ç»Ÿä¸€çš„æµ‹è¯•é—®é¢˜ï¼ˆéœ€è¦å®æ—¶ä¿¡æ¯ï¼‰
TEST_QUESTION = "ä»Šå¤©ï¼ˆ2026å¹´1æœˆ15æ—¥ï¼‰æ—¥æœ¬ç­‘æ³¢çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿæ°”æ¸©å¤šå°‘åº¦ï¼Ÿ"


async def test_search_model_with_convenience_function():
    """æµ‹è¯• 1ï¼šä½¿ç”¨ä¾¿æ·å‡½æ•° + æœç´¢æ¨¡å‹"""
    print("=" * 80)
    print("ã€æµ‹è¯• 1ã€‘ä½¿ç”¨ execute_search_call ä¾¿æ·å‡½æ•°ï¼ˆè‡ªåŠ¨å¯ç”¨æœç´¢ï¼‰")
    print("=" * 80)
    print(f"ğŸ“ æµ‹è¯•é—®é¢˜: {TEST_QUESTION}")
    
    try:
        response, stats = await execute_search_call(TEST_QUESTION)
        print(f"\nâœ… æœç´¢æˆåŠŸ")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}")
        print(f"ğŸ” æ˜¯å¦ä½¿ç”¨æœç´¢æ¨¡å‹: {stats.get('is_search_model', False)}")
        print(f"\nğŸ’¬ AIå›å¤:\n{response}\n")
    except Exception as e:
        print(f"âŒ æœç´¢å¤±è´¥: {e}\n")


async def test_search_model_manual_config():
    """æµ‹è¯• 2ï¼šæ‰‹åŠ¨é…ç½®æœç´¢æ¨¡å‹"""
    print("=" * 80)
    print("ã€æµ‹è¯• 2ã€‘æ‰‹åŠ¨é…ç½® gpt-4o-search-previewï¼ˆå¯ç”¨æœç´¢ï¼‰")
    print("=" * 80)
    
    try:
        # åˆ›å»ºé…ç½®ï¼Œæ˜¾å¼å¯ç”¨æœç´¢
        config = LLMConfig(
            model="gpt-4o-search-preview",
            enable_search=True,
            temperature=0.3
        )
        
        # æ„å»ºæ¶ˆæ¯
        messages = format_messages_for_llm(
            "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ï¼Œè¯·æä¾›å‡†ç¡®çš„ä¿¡æ¯ã€‚"
        )
        messages.append(HumanMessage(content=TEST_QUESTION))
        
        # è°ƒç”¨
        response, stats = await execute_llm_call(messages, config)
        print(f"\nâœ… è°ƒç”¨æˆåŠŸ")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}")
        print(f"ğŸ” æ˜¯å¦ä½¿ç”¨æœç´¢æ¨¡å‹: {stats.get('is_search_model', False)}")
        print(f"\nğŸ’¬ AIå›å¤:\n{response}\n")
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤±è´¥: {e}\n")


async def test_non_search_model():
    """æµ‹è¯• 3ï¼šæ™®é€šæ¨¡å‹ï¼ˆå¯¹æ¯”ç»„ - æ— è”ç½‘æœç´¢ï¼‰"""
    print("=" * 80)
    print("ã€æµ‹è¯• 3ã€‘æ™®é€šæ¨¡å‹ gpt-4oï¼ˆç¦ç”¨æœç´¢ - å¯¹æ¯”ç»„ï¼‰")
    print("=" * 80)
    print(f"ğŸ“ æµ‹è¯•é—®é¢˜: {TEST_QUESTION}")
    
    try:
        # ä½¿ç”¨æ™®é€šæ¨¡å‹ï¼Œç¦ç”¨æœç´¢
        config = LLMConfig(
            model="gpt-4o",
            enable_search=False,
            temperature=0.3  # ä½¿ç”¨ç›¸åŒçš„ temperature
        )
        
        messages = format_messages_for_llm("ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ï¼Œè¯·æä¾›å‡†ç¡®çš„ä¿¡æ¯ã€‚")
        messages.append(HumanMessage(content=TEST_QUESTION))
        
        response, stats = await execute_llm_call(messages, config)
        print(f"\nâœ… è°ƒç”¨æˆåŠŸ")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}")
        print(f"ğŸ” æ˜¯å¦ä½¿ç”¨æœç´¢æ¨¡å‹: {stats.get('is_search_model', False)}")
        print(f"\nğŸ’¬ AIå›å¤:\n{response}\n")
        print("âš ï¸  æ³¨æ„ï¼šæ™®é€šæ¨¡å‹æ— æ³•è·å–å®æ—¶ä¿¡æ¯ï¼Œå¯èƒ½ä¼šæ‹’ç»å›ç­”æˆ–æä¾›è¿‡æ—¶ä¿¡æ¯")
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤±è´¥: {e}\n")


async def main():
    print("\n" + "ğŸ”" * 40)
    print("LLM è”ç½‘æœç´¢åŠŸèƒ½å¯¹æ¯”æµ‹è¯•")
    print("ğŸ”" * 40 + "\n")
    
    print(f"ğŸ“‹ ç»Ÿä¸€æµ‹è¯•é—®é¢˜: {TEST_QUESTION}")
    print("ğŸ¯ ç›®çš„: å¯¹æ¯”æœ‰æ— æœç´¢åŠŸèƒ½çš„æ¨¡å‹å“åº”å·®å¼‚\n")
    
    # æµ‹è¯• 1: ä½¿ç”¨ä¾¿æ·å‡½æ•°ï¼ˆæœç´¢æ¨¡å‹ï¼‰
    await test_search_model_with_convenience_function()
    
    await asyncio.sleep(2)
    
    # æµ‹è¯• 2: æ‰‹åŠ¨é…ç½®ï¼ˆæœç´¢æ¨¡å‹ï¼‰
    await test_search_model_manual_config()
    
    await asyncio.sleep(2)
    
    # æµ‹è¯• 3: æ™®é€šæ¨¡å‹ï¼ˆå¯¹æ¯”ç»„ï¼‰
    await test_non_search_model()
    
    print("=" * 80)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)
    
    print("\nğŸ“Š é¢„æœŸç»“æœå¯¹æ¯”:")
    print("  - æµ‹è¯• 1 & 2ï¼ˆæœç´¢æ¨¡å‹ï¼‰ï¼šåº”è¯¥èƒ½å¤Ÿè·å–å®æ—¶å¤©æ°”æ•°æ®ï¼Œç»™å‡ºå‡†ç¡®çš„æ¸©åº¦")
    print("  - æµ‹è¯• 3ï¼ˆæ™®é€šæ¨¡å‹ï¼‰ï¼š   æ— æ³•è·å–å®æ—¶ä¿¡æ¯ï¼Œå¯èƒ½æ‹’ç»å›ç­”æˆ–è¯´æ˜æ— æ³•è®¿é—®å®æ—¶æ•°æ®")
    
    print("\nğŸ’¡ åœ¨é¡¹ç›®ä¸­å¯ç”¨æœç´¢åŠŸèƒ½:")
    print("  1. åœ¨ .env ä¸­è®¾ç½® OPENAI_MODEL=gpt-4o-search-preview")
    print("  2. åœ¨ .env ä¸­è®¾ç½® ENABLE_LLM_SEARCH=True")
    print("  3. é‡å¯æœåŠ¡ python main.py")
    print("  4. æ‰€æœ‰ LLM è°ƒç”¨å°†è‡ªåŠ¨ä½¿ç”¨è”ç½‘æœç´¢åŠŸèƒ½")
    print("")


if __name__ == "__main__":
    asyncio.run(main())


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM å·¥å…·å‡½æ•° - ä½¿ç”¨åŸç”ŸOpenAI APIå¯ç”¨æœç´¢åŠŸèƒ½
å‚è€ƒ cognitive_model/agents/llm_utils.py
"""
import logging
from typing import Dict, Any, List, Optional, Callable, Awaitable, Union
from openai import AsyncOpenAI  # ä½¿ç”¨å¼‚æ­¥OpenAIå®¢æˆ·ç«¯
from openai.types.chat import ChatCompletionMessageParam
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

from config import settings

logger = logging.getLogger(__name__)

class LLMConfig:
    """LLM é…ç½®ç±» - æ”¯æŒæœç´¢åŠŸèƒ½"""
    def __init__(
        self,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        enable_search: Optional[bool] = None,  # æ˜¯å¦å¯ç”¨æœç´¢ï¼ˆNone=è‡ªåŠ¨æ ¹æ®é…ç½®ï¼‰
        search_options: Optional[Dict] = None  # æœç´¢é€‰é¡¹é…ç½®
    ):
        # å¦‚æœæœªæŒ‡å®šï¼Œæ ¹æ®å…¨å±€é…ç½®å†³å®š
        self.enable_search = enable_search if enable_search is not None else settings.ENABLE_LLM_SEARCH
        
        # æ ¹æ® enable_search å†³å®šä½¿ç”¨å“ªä¸ªæ¨¡å‹
        if model:
            # æ˜¾å¼æŒ‡å®šæ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨
            self.model = model
        elif self.enable_search is True:
            # å¯ç”¨æœç´¢ï¼Œä½¿ç”¨æœç´¢æ¨¡å‹
            self.model = settings.OPENAI_SEARCH_MODEL
        elif self.enable_search is False:
            # ç¦ç”¨æœç´¢ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹
            self.model = settings.OPENAI_BASE_MODEL
            # print(f"ğŸ”§ ç¦ç”¨æœç´¢ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹: {self.model}")
        else:
            # æœªæŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
            self.model = settings.OPENAI_MODEL
        
        self.temperature = temperature or settings.OPENAI_TEMPERATURE
        self.max_tokens = max_tokens
        self.search_options = search_options or {}

def _convert_to_openai_format(messages: Union[List, List[ChatCompletionMessageParam]]) -> List[ChatCompletionMessageParam]:
    """
    å°†LangChainæ¶ˆæ¯æˆ–å­—å…¸æ¶ˆæ¯è½¬æ¢ä¸ºOpenAI APIæ ¼å¼
    æ”¯æŒæ··åˆè¾“å…¥æ ¼å¼
    """
    openai_messages: List[ChatCompletionMessageParam] = []
    
    for msg in messages:
        # å¦‚æœå·²ç»æ˜¯å­—å…¸æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
        if isinstance(msg, dict):
            openai_messages.append(msg)  # type: ignore
        # å¦‚æœæ˜¯ LangChain æ¶ˆæ¯å¯¹è±¡
        elif hasattr(msg, 'content'):
            if isinstance(msg, SystemMessage):
                openai_messages.append({"role": "system", "content": msg.content})
            elif isinstance(msg, HumanMessage):
                openai_messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                openai_messages.append({"role": "assistant", "content": msg.content})
            else:
                # å…¶ä»–ç±»å‹ï¼Œå°è¯•ä» type å±æ€§è·å–
                role = getattr(msg, 'type', 'user')
                if role == 'human':
                    role = 'user'
                elif role == 'ai':
                    role = 'assistant'
                openai_messages.append({"role": role, "content": msg.content})  # type: ignore
    
    return openai_messages


def format_messages_for_llm(
    system_prompt: str,
    history: Optional[List[Dict[str, str]]] = None
) -> List:
    """
    æ ¼å¼åŒ–æ¶ˆæ¯ä¸º LangChain æ ¼å¼ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
    è¿”å› LangChain æ¶ˆæ¯å¯¹è±¡åˆ—è¡¨
    """
    messages = [SystemMessage(content=system_prompt)]
    
    if history:
        for msg in history:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            if role == "user":
                messages.append(HumanMessage(content=content))
            elif role == "assistant":
                messages.append(AIMessage(content=content))
    
    return messages

def format_config_for_llm(model_config: Optional[Dict[str, Any]] = None) -> LLMConfig:
    """æ ¼å¼åŒ– LLM é…ç½®"""
    if model_config:
        return LLMConfig(
            model=model_config.get("model_name"),
            temperature=model_config.get("temperature"),
            max_tokens=model_config.get("max_tokens"),
            enable_search=model_config.get("enable_search"),  # None ä¼šä½¿ç”¨å…¨å±€é…ç½®
            search_options=model_config.get("search_options")
        )
    return LLMConfig()

async def execute_llm_call(
    messages: Union[List, List[ChatCompletionMessageParam]],
    config: Optional[LLMConfig] = None,
    stream: bool = False,
    stream_callback: Optional[Callable[[str], Awaitable[None]]] = None
) -> tuple[str, Dict[str, Any]]:
    """
    æ‰§è¡Œ LLM è°ƒç”¨ - ä½¿ç”¨åŸç”ŸOpenAI APIå¹¶æ”¯æŒæœç´¢
    å…¼å®¹ LangChain æ¶ˆæ¯æ ¼å¼å’Œ OpenAI å­—å…¸æ ¼å¼
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨ï¼ˆæ”¯æŒ LangChain æ ¼å¼æˆ– OpenAI æ ¼å¼ï¼‰
        config: LLM é…ç½®
        stream: æ˜¯å¦æµå¼è¿”å›
        stream_callback: æµå¼å›è°ƒå‡½æ•°
        
    Returns:
        tuple: (response_content, stats_dict)
    """
    config = config or LLMConfig()
    
    # åˆå§‹åŒ–å¼‚æ­¥OpenAIå®¢æˆ·ç«¯
    client = AsyncOpenAI(
        api_key=settings.OPENAI_API_KEY,
        base_url=settings.OPENAI_BASE_URL,
    )
    
    try:
        # è‡ªåŠ¨è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸º OpenAI API æ ¼å¼
        openai_messages = _convert_to_openai_format(messages)
        
        # å‡†å¤‡APIè°ƒç”¨å‚æ•°
        api_params: Dict[str, Any] = {
            "model": config.model,
            "messages": openai_messages,
            "temperature": config.temperature,
        }
        
        # åªåœ¨æœ‰å€¼æ—¶æ·»åŠ  max_tokens
        if config.max_tokens:
            api_params["max_tokens"] = config.max_tokens
        
        # å¦‚æœå¯ç”¨æœç´¢ï¼Œå¯ä»¥æ·»åŠ æœç´¢é€‰é¡¹ï¼ˆå¦‚æœAPIæ”¯æŒï¼‰
        if config.enable_search:
            if config.search_options:
                api_params["extra"] = config.search_options
            # print(f"ğŸ” ä½¿ç”¨æœç´¢æ¨¡å‹: {config.model}")
        
        if stream:
            # å¼‚æ­¥æµå¼è°ƒç”¨
            chunks = []
            stream_response = await client.chat.completions.create(
                **api_params,
                stream=True
            )
            
            async for chunk in stream_response:
                if chunk.choices and chunk.choices[0].delta.content:
                    chunk_content = chunk.choices[0].delta.content
                    chunks.append(chunk_content)
                    if stream_callback:
                        await stream_callback(chunk_content)
            
            response_content = "".join(chunks)
        else:
            # éæµå¼è°ƒç”¨
            response = await client.chat.completions.create(**api_params)
            response_content = response.choices[0].message.content or ""
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "model": config.model,
            "enable_search": config.enable_search,
            "is_search_model": config.enable_search,  # ä¸ enable_search ä¿æŒä¸€è‡´
            "response_length": len(response_content)
        }
        
        return response_content, stats
        
    except Exception as e:
        logger.error(f"LLM è°ƒç”¨å¤±è´¥: {e}", exc_info=True)
        raise

# æ–°å¢ï¼šä¸“é—¨ç”¨äºæœç´¢çš„ä¾¿æ·å‡½æ•°
async def execute_search_call(
    query: str,
    system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ï¼Œè¯·åŸºäºç½‘ç»œæœç´¢æä¾›å‡†ç¡®ã€åŠæ—¶çš„ä¿¡æ¯ã€‚",
    search_options: Optional[Dict] = None,
    temperature: float = 0.3,
    max_tokens: int = 2000
) -> tuple[str, Dict[str, Any]]:
    """
    æ‰§è¡Œå¸¦æœ‰è”ç½‘æœç´¢çš„ LLM è°ƒç”¨
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        search_options: æœç´¢é€‰é¡¹é…ç½®
        temperature: æ¸©åº¦å‚æ•°ï¼ˆé»˜è®¤ 0.3ï¼Œæ›´ç²¾ç¡®ï¼‰
        max_tokens: æœ€å¤§ token æ•°ï¼ˆé»˜è®¤ 2000ï¼‰
        
    Returns:
        tuple: (response_content, stats_dict)
    """
    config = LLMConfig(
        model="gpt-4o-search-preview",  # ä½¿ç”¨æœç´¢æ¨¡å‹
        enable_search=True,
        search_options=search_options,
        temperature=temperature,
        max_tokens=max_tokens
    )
    
    messages = format_messages_for_llm(system_prompt, [])
    messages.append(HumanMessage(content=query))
    
    return await execute_llm_call(messages, config)